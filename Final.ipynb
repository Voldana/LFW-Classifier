{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Voldana\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\voldana\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.22.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\voldana\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Voldana\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\voldana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.22.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Voldana\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = r\"D:\\University Courses\\Computational AI\\Datasets\\Final\"\n",
    "lfw_folder = r\"D:\\University Courses\\Computational AI\\Datasets\\Final\\lfw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(fileName,index):\n",
    "    \n",
    "    path = lfw_folder+\"\\\\\" +fileName+\"\\\\\"+fileName+\"_\"+change_name_format(index)+\".txt\"    \n",
    "    raw_data = replace_signs(path).split()\n",
    "    arr = np.zeros((1,512))\n",
    "    for i in range(0,512):\n",
    "        arr[0][i] = np.float32(raw_data[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_signs(path):\n",
    "    file=open(path).readlines()\n",
    "    file=str(file).replace(r\"\\n', '  \",\" \")\n",
    "    file=file.replace(\",\",\"\")\n",
    "    file=file.replace(\"'\",\"\")\n",
    "    file=file.replace(\"[\",\"\")\n",
    "    file=file.replace(\"]\",\"\")\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name_format(index):\n",
    "    temp_num_0 = int(index / 1000)\n",
    "    temp_num = index % 1000\n",
    "    temp_num_1 = int(temp_num / 100)\n",
    "    temp_num = temp_num % 100\n",
    "    temp_num_2 = int(temp_num / 10)\n",
    "    temp_num_3 = temp_num % 10 \n",
    "    new_str = \"{}{}{}{}\".format(temp_num_0,temp_num_1,temp_num_2,temp_num_3)\n",
    "    \n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a data set and a label set for both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_set(data_list,number_of_classes):\n",
    "    data_set=np.zeros((number_of_classes * 2,1024),dtype=np.float32)\n",
    "    label_set=np.zeros(number_of_classes * 2,dtype=int)\n",
    "    # make_subsets(data_list,data_set,label_set,0,number_of_classes,1)\n",
    "    # make_subsets(data_list,data_set,label_set,number_of_classes + 1,number_of_classes,0)\n",
    "    for i in range(0,number_of_classes):\n",
    "        temp_str = data_list[i]\n",
    "        temp_str = temp_str.replace(\"\\n\",\"\\t\")\n",
    "        splitted = temp_str.split(\"\\t\")\n",
    "        first_feature=extract_feature(splitted[0],int(splitted[1]))\n",
    "        second_feature=extract_feature(splitted[0],int(splitted[2]))\n",
    "        data_set[i] = np.concatenate((first_feature,second_feature),axis=1)\n",
    "        label_set[i] = 1\n",
    "\n",
    "    for i in range(number_of_classes,number_of_classes * 2):\n",
    "        temp_str = data_list[i]\n",
    "        temp_str = temp_str.replace(\"\\n\",\"\\t\")\n",
    "        splitted = temp_str.split(\"\\t\")\n",
    "        first_feature=extract_feature(splitted[0],int(splitted[1]))\n",
    "        second_feature=extract_feature(splitted[2],int(splitted[3]))\n",
    "        data_set[i] = np.concatenate((first_feature,second_feature),axis=1)\n",
    "        label_set[i] = 0  \n",
    "        \n",
    "    return data_set,label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subsets(data_list,data_set,label_set,start,number_of_classes,value):\n",
    "    for i in range(start,start + number_of_classes - 5):\n",
    "        temp_str = data_list[i]\n",
    "        temp_str = temp_str.replace(\"\\n\",\"\\t\")\n",
    "        splitted = temp_str.split(\"\\t\")\n",
    "        first_feature=extract_feature(splitted[0],int(splitted[1]))\n",
    "        second_feature=extract_feature(splitted[0],int(splitted[2]))\n",
    "        data_set[i] = np.concatenate((first_feature,second_feature),axis=1)\n",
    "        label_set[i] = value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = open(dataset_folder+\"\\\\pairsDevTrain.txt\").readlines()\n",
    "train_class_count = train_list.pop(0)\n",
    "train_data_set,train_label_set=make_set(train_list,int(train_class_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = open(dataset_folder+\"\\\\pairsDevTest.txt\").readlines()\n",
    "test_class_count = test_list.pop(0)\n",
    "test_data_set,test_label_set=make_set(test_list,int(test_class_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing bootsrap bagging with DT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking bootstrap accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.578\n"
     ]
    }
   ],
   "source": [
    "bagging_pred=bagging_clf.predict(test_data_set)\n",
    "bagging_acc=accuracy_score(bagging_pred,test_label_set)\n",
    "print(bagging_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gonna try increasing the max_features but this might cause overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=10,max_features=512).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579\n"
     ]
    }
   ],
   "source": [
    "bagging_pred=bagging_clf.predict(test_data_set)\n",
    "bagging_acc=accuracy_score(bagging_pred,test_label_set)\n",
    "print(bagging_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even if we use all of the features it doesn't improve accuracy by that much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok let's increase the number of estimators and keep the max_features low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=1000,max_features=100).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577\n"
     ]
    }
   ],
   "source": [
    "bagging_pred=bagging_clf.predict(test_data_set)\n",
    "bagging_acc=accuracy_score(bagging_pred,test_label_set)\n",
    "print(bagging_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy improved by 4% or so, let's try 500 estimators with default modes on max_samples and max_features with 8 as random state\n",
    "\n",
    "... this took longer than i expected to run it took 11 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=500, random_state=8).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712\n"
     ]
    }
   ],
   "source": [
    "bagging_pred=bagging_clf.predict(test_data_set)\n",
    "bagging_acc=accuracy_score(bagging_pred,test_label_set)\n",
    "print(bagging_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to increase the accuracy by about 13% so we might be on the right track the only problem is training time takes almost 100 times more than before so here I'm gonna try using the default values for max_sample and max_features but more estimators lets say 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=1000, random_state=8).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "bagging_pred=bagging_clf.predict(test_data_set)\n",
    "bagging_acc=accuracy_score(bagging_pred,test_label_set)\n",
    "print(bagging_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok after 23 minutes of training we only say 0.8% improvement in our accuracy so let's leave the bagging with DT classifier at here, we might be able to get more accuracy by changing the parameters but i don't think that it'd be that significant.. however this is the best accuracy we managed to reach so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try bagging with SVM classifier this time we're gonna use 100 max samples and 100 for max features and 10 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = BaggingClassifier(base_estimator=SVC(),n_estimators=10,max_samples=100,max_features=100).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558\n"
     ]
    }
   ],
   "source": [
    "svm_pred=svm_clf.predict(test_data_set)\n",
    "svm_acc=accuracy_score(svm_pred,test_label_set)\n",
    "print(svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that's not very satisfying so let's try increasing the estimators and leaving max saple and max features to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = BaggingClassifier(base_estimator=SVC(),n_estimators=100).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729\n"
     ]
    }
   ],
   "source": [
    "svm_pred=svm_clf.predict(test_data_set)\n",
    "svm_acc=accuracy_score(svm_pred,test_label_set)\n",
    "print(svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that's a huge improvement and better than our DT classifier by a littSle bit let's try lowering the estimators again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = BaggingClassifier(base_estimator=SVC(),n_estimators=20).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729\n"
     ]
    }
   ],
   "source": [
    "svm_pred=svm_clf.predict(test_data_set)\n",
    "svm_acc=accuracy_score(svm_pred,test_label_set)\n",
    "print(svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok changing the estimators didn't lead to anything\n",
    "Now let's change the SVM gamma to see if we can get an improvement in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = BaggingClassifier(base_estimator=SVC(gamma = 0.0003),n_estimators=20).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754\n"
     ]
    }
   ],
   "source": [
    "svm_pred=svm_clf.predict(test_data_set)\n",
    "svm_acc=accuracy_score(svm_pred,test_label_set)\n",
    "print(svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok the improvement is decent at this point let's try different gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = BaggingClassifier(base_estimator=SVC(gamma = 0.0002),n_estimators=20).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n"
     ]
    }
   ],
   "source": [
    "svm_pred=svm_clf.predict(test_data_set)\n",
    "svm_acc=accuracy_score(svm_pred,test_label_set)\n",
    "print(svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the best accuracy we've been able to reach so 76.4% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try bagging with MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = BaggingClassifier(base_estimator= MLPClassifier(),n_estimators=10).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761\n"
     ]
    }
   ],
   "source": [
    "mlp_pred=mlp_clf.predict(test_data_set)\n",
    "mlp_acc=accuracy_score(mlp_pred,test_label_set)\n",
    "print(mlp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok looks like we're on to a good start with MLP let's increase the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = BaggingClassifier(base_estimator= MLPClassifier(),n_estimators=20).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774\n"
     ]
    }
   ],
   "source": [
    "mlp_pred=mlp_clf.predict(test_data_set)\n",
    "mlp_acc=accuracy_score(mlp_pred,test_label_set)\n",
    "print(mlp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better than our SVM classifier since increasing estimators improved accuracy by 1.5% we're gonna keep doing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = BaggingClassifier(base_estimator= MLPClassifier(),n_estimators=30).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786\n"
     ]
    }
   ],
   "source": [
    "mlp_pred=mlp_clf.predict(test_data_set)\n",
    "mlp_acc=accuracy_score(mlp_pred,test_label_set)\n",
    "print(mlp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of estimators is improving the accuracy.. however it's taking alot of CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = BaggingClassifier(base_estimator= MLPClassifier(),n_estimators=50).fit(train_data_set, train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n"
     ]
    }
   ],
   "source": [
    "mlp_pred=mlp_clf.predict(test_data_set)\n",
    "mlp_acc=accuracy_score(mlp_pred,test_label_set)\n",
    "print(mlp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok seems like increasing the number of estimators no longer improves accuracy, MLP might be the algorithem that gives us the best accuracy with bagging however it takes alot of my system's resources so i can't modify the parameters that much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Try Random forest to see what it has to offer us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=10).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n"
     ]
    }
   ],
   "source": [
    "rf_pred=rf_clf.predict(test_data_set)\n",
    "rf_acc=accuracy_score(rf_pred,test_label_set)\n",
    "print(rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631\n"
     ]
    }
   ],
   "source": [
    "rf_pred=rf_clf.predict(test_data_set)\n",
    "rf_acc=accuracy_score(rf_pred,test_label_set)\n",
    "print(rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we saw a decent improvement by increasing the number of estimators so let's do that again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=500).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687\n"
     ]
    }
   ],
   "source": [
    "rf_pred=rf_clf.predict(test_data_set)\n",
    "rf_acc=accuracy_score(rf_pred,test_label_set)\n",
    "print(rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok 5% isn't that bad we might be able to reach more accuracy than bagging with SVM so let's add more estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=5000).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "rf_pred=rf_clf.predict(test_data_set)\n",
    "rf_acc=accuracy_score(rf_pred,test_label_set)\n",
    "print(rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of estimators improved random forest accuracy by around 4% so let's try giving it more estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=10000).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726\n"
     ]
    }
   ],
   "source": [
    "rf_pred=rf_clf.predict(test_data_set)\n",
    "rf_acc=accuracy_score(rf_pred,test_label_set)\n",
    "print(rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok it seems random forest accuracy caps at 73% for us which is still less than our bagging method with SVM let's move on to the next method of ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost_clf = AdaBoostClassifier().fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522\n"
     ]
    }
   ],
   "source": [
    "ada_boost_pred=ada_boost_clf.predict(test_data_set)\n",
    "ada_boost_acc=accuracy_score(ada_boost_pred,test_label_set)\n",
    "print(ada_boost_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increasing the number of estimators to see if it leads to any improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost_clf = AdaBoostClassifier(n_estimators = 500).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531\n"
     ]
    }
   ],
   "source": [
    "ada_boost_pred=ada_boost_clf.predict(test_data_set)\n",
    "ada_boost_acc=accuracy_score(ada_boost_pred,test_label_set)\n",
    "print(ada_boost_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy didn't change much by increasing the estimator numbers so let's increasing max depth of our DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost_clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 2)).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "ada_boost_pred=ada_boost_clf.predict(test_data_set)\n",
    "ada_boost_acc=accuracy_score(ada_boost_pred,test_label_set)\n",
    "print(ada_boost_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well since the improvement is quite decent let's try increasing max_depth again to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost_clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 3)).fit(train_data_set,train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589\n"
     ]
    }
   ],
   "source": [
    "ada_boost_pred=ada_boost_clf.predict(test_data_set)\n",
    "ada_boost_acc=accuracy_score(ada_boost_pred,test_label_set)\n",
    "print(ada_boost_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing max_depth might have caused overfitting in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging with DT : Best results were 72% accuracy using 1000 estimators and 8 random states\n",
    "------------\n",
    "Bagging with SVM: Best results were 76.4% accuracy using 0.0002 as gamma value and 20 estimators\n",
    "------------\n",
    "Bagging with MLP: Best results were 78.6% accuracy using 30 estimators\n",
    "------------\n",
    "Random Forest: Best results were 73% accuracy using 5000 estimators\n",
    "------------\n",
    "AdaBoosting : best results were 60% accuracy using a DT classifier with max_depth of 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now bagging using MLP classifier seems like the best option we have it has better accuracy than others also it takes barely any time to test on the test data unlike bagging with SVM which takes equal time to train or to test\n",
    "Overall the accuracy of the mentioned algorithems can be improved by testing different configurations (specially for those using DT classifier)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7202ae67a81950729b30ad1d27c95d588677893e69ab877ffc2f51431a20d22"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
